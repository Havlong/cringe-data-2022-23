# UralOpenHackathon23

При запуске не забудьте поместить датасет `test_well.csv` в папку `/tf/datasets/` или изменить путь в Jupyter блокноте

## Что было использовано, но не попало в блокнот

### Альтернативные регрессоры
В процессе решения пробовались альтернативные регрессоры в комбинации с `GridSearchCV`, которые не дали улучшения

```python
regressor_params = {
    'alpha': np.logspace(-6, 2, 200)
}

model = GridSearchCV(Ridge(), regressor_params)
```

```python
regressor_params = {
    'alpha': np.logspace(-6, 2, 200)
}

model = GridSearchCV(ElasticNet(), regressor_params)
```

### Выбор параметров
Для выбора параметров использовался полный перебор в сочетании с линейным регрессором

```python
model_cols = ['FREQ_HZ', 'COS_PHI', 'ACTIV_POWER', 'PINP', 'PLIN']
n = len(model_cols)
for i in range(1, 2**n):
    feats = [model_cols[j] for j in range(n) if (1 << j) & i != 0]
    print(feats)
    X = used_df[feats]
    Y = used_df["LIQ_RATE"]
    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=42)
    model = LinearRegression()
    model.fit(X_train, Y_train)
    Y_predicted = model.predict(X_test)
    print('R2 Score:', metrics.r2_score(Y_test, Y_predicted))
```

## Идеи

### Baseline problem
В авторском решении, которое надо было обогнать, очень неправильно сжимались целевые параметры, так как замеров в день могло быть больше одного.
Из-за этого было сложно сравнить свой `r2 score` с базовым решением.
С целью сохранения интегрированности, я не изменял процесса генерации выборки и процесс аггрегации пропущенных значений.

### Что использовать в качестве входа
Возможно более правильным будет использовать при сравнении с целевым значением лишь последнее измерение.
Для этого нужно лишь заменить фрагмент `df = df.groupby("JOIN_DT").agg("mean")` на `df.dropna(inplace=True)`, и моё решение оставит лишь последние значения.
Но этот вариант я абсолютно не исследовал...

### Более аккуратная замена пропущенных измерений
У меня не хватило времени заменить свой Imputer с последними значениями на линейные фрагменты.
Одним из вариантов интерполяции могли служить простые линейные функции между пропущенными значениями.
Однако параметром должно выступать __время__, а не позиция в датасете.

Интересным вариантом возможно могло стать использование `IterativeImputer`, который выстраивает регрессию по известным значениям и заполняет пропущенные.
Однако с этим инструментом я не работал и решил не рисковать.

### Нейронные сети
Могут легко получить высокий результат за счёт высокого оверфиттинга, однако в реальных условиях вряд ли окажутся эффективными.
Вопрос стоит в отсутствии тестового датасета, т.к. в такие модели утекают данные из датасета малого размера.

### Отсечение выбросов
Сразу после слепого применения трёх сигм, несколько просел `r2 score`, возможно были отсечены данные, влияющие на предсказание.
Как альтернативы для детекции рассматривались, но не использовались модели `IsolationForest` и `LocalOutlierFactor` из `sklearn`.

### Полиномиальные признаки
Я не тестировал альтернативные модели и наборы входных измерений с выявлением квадратичных/кубических признаков.
Возможно это имеет смысл и может улучшить решение дальше, но я добавил их в последний момент для того, чтобы получить ___0.0001___ к `r2 score`

### Измерения после последнего целевого значения
Были удалены, возможно при обучении стоило их использовать, если пытаться заполнять пустые значения интерполируемыми.
Это опять же имеет смысл при другом варианте аггрегирования, до которого я не догадался, например:

### Угасающие последние измерения
Возможно на вход для вычисления целевого значения в какой-то момент времени оптимальнее использовать интерполированные значения за последние несколько интервалов времени.
Например использовать последнее значение, интерполированное значение на 5 минут ранее / 2, интерполированное значение на 10 минут ранее / 4, на 15 / 8, и т.д.

## Примечание
Я за этот восьмичасовой труд получил ничего, несмотря на попытку исправить ошибки организаторов.
Так что смело могу сказать:
> Соревнование - дерьмо

Материалы и репозиторий оставлю для будущего себя и других людей, которым приходится обрабатывать странные данные
